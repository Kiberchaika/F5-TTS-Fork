{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k4/Python/F5-TTS-Fork/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-17 03:24:10,746 - INFO - PyTorch version 2.5.1+cu121 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Vocos from huggingface charactr/vocos-mel-24khz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k4/Python/F5-TTS-Fork/.venv/lib/python3.10/site-packages/vocos/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocab :  /home/k4/Python/F5-TTS-Fork/src/f5_tts/infer/examples/vocab.txt\n",
      "tokenizer :  custom\n",
      "model :  /home/k4/Python/F5-TTS-Fork/ckpts/russian_coarse_16nov/model_0230.pt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - system\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import os\n",
    "import re\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tomli\n",
    "from cached_path import cached_path\n",
    "\n",
    "import cyrtranslit\n",
    "\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_single_process,\n",
    "    load_model,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    preprocess_ref_audio_text_segment,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "\n",
    "\n",
    "from f5_tts.model.utils import (\n",
    "    default,\n",
    "    exists,\n",
    "    lens_to_mask,\n",
    "    list_str_to_idx,\n",
    "    list_str_to_tensor,\n",
    "    mask_from_frac_lengths,\n",
    ")\n",
    "\n",
    "from f5_tts.model import DiT, UNetT\n",
    "\n",
    "# Direct variable assignments instead of command line arguments\n",
    "model = \"F5-TTS\"\n",
    "# ckpt_file = \"/home/k4/Python/F5-TTS-Fork/ckpts/russian_dataset_ft_translit_pinyin/model_last.pt\"\n",
    "# ckpt_file = \"/home/k4/Python/F5-TTS-Fork/ckpts/russian_coarse_16nov/model_40000.pt\"\n",
    "ckpt_file = \"/home/k4/Python/F5-TTS-Fork/ckpts/russian_coarse_16nov/model_0230.pt\"\n",
    "\n",
    "# Default configurations\n",
    "config_path = os.path.join(files(\"f5_tts\").joinpath(\"infer/examples/basic\"), \"basic.toml\")\n",
    "config = tomli.load(open(config_path, \"rb\"))\n",
    "\n",
    "# Additional settings with default values\n",
    "vocab_file = \"\"\n",
    "output_dir = config[\"output_dir\"]\n",
    "remove_silence = config.get(\"remove_silence\", False)\n",
    "speed = 1.0\n",
    "vocoder_name = \"vocos\"\n",
    "load_vocoder_from_local = False\n",
    "wave_path = Path(output_dir) / \"infer_cli_out.wav\"\n",
    "\n",
    "# Vocoder settings\n",
    "if vocoder_name == \"vocos\":\n",
    "    vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\n",
    "elif vocoder_name == \"bigvgan\":\n",
    "    vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\n",
    "mel_spec_type = vocoder_name\n",
    "\n",
    "vocoder = load_vocoder(vocoder_name=mel_spec_type, is_local=load_vocoder_from_local, local_path=vocoder_local_path)\n",
    "\n",
    "# Model configuration\n",
    "if model == \"F5-TTS\":\n",
    "    model_cls = DiT\n",
    "    model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
    "    # if not ckpt_file:\n",
    "    #     if vocoder_name == \"vocos\":\n",
    "    #         repo_name = \"F5-TTS\"\n",
    "    #         exp_name = \"F5TTS_Base\"\n",
    "    #         ckpt_step = 1200000\n",
    "    #         ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\n",
    "    #     elif vocoder_name == \"bigvgan\":\n",
    "    #         repo_name = \"F5-TTS\"\n",
    "    #         exp_name = \"F5TTS_Base_bigvgan\"\n",
    "    #         ckpt_step = 1250000\n",
    "    #         ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.pt\"))\n",
    "\n",
    "\n",
    "ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - main process\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_mel_spectrogram(audio_path, save_path, title):\n",
    "    y, sr = librosa.load(audio_path, sr=None)  # Load audio\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to dB scale\n",
    "\n",
    "    # Plot and save the mel-spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis=\"time\", y_axis=\"mel\", fmax=8000, cmap=\"magma\")\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main_process(ref_audio, ref_text, text_gen, model_obj, mel_spec_type, remove_silence, speed, end_time=None,cfg_strength=2.0, nfe_step=32, start_step=0, end_step=32):\n",
    "    # Single voice configuration\n",
    "    main_voice = {\"ref_audio\": ref_audio, \"ref_text\": ref_text}\n",
    "    ref_audio, ref_text = preprocess_ref_audio_text_segment(main_voice[\"ref_audio\"], main_voice[\"ref_text\"], end_time=end_time, clip_short=False, bypass_cache=True)\n",
    "    print(\"Ref_audio:\", ref_audio)\n",
    "    print(\"Ref_text:\", ref_text)\n",
    "\n",
    "    # Generate audio\n",
    "    audio, final_sample_rate, _, trajectory = infer_single_process(\n",
    "        ref_audio, ref_text, text_gen, model_obj, vocoder, mel_spec_type=mel_spec_type, nfe_step=nfe_step, cfg_strength=cfg_strength, speed=speed, start_step=start_step, end_step=end_step\n",
    "    )\n",
    "\n",
    "    # Save the generated wave\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(wave_path, \"wb\") as f:\n",
    "        sf.write(f.name, audio, final_sample_rate)\n",
    "        if remove_silence:\n",
    "            remove_silence_for_generated_wav(f.name)\n",
    "        print(f\"Saved generated audio to {f.name}\")\n",
    "\n",
    "    # Save mel-spectrograms\n",
    "    ref_mel_path = Path(output_dir) / \"ref_mel.png\"\n",
    "    gen_mel_path = Path(output_dir) / \"gen_mel.png\"\n",
    "\n",
    "    save_mel_spectrogram(ref_audio, ref_mel_path, \"Reference Audio Mel-Spectrogram\")\n",
    "    save_mel_spectrogram(wave_path, gen_mel_path, \"Generated Audio Mel-Spectrogram\")\n",
    "\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройки генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from ref_data_cache.json\n",
      "found 766 base tracks with lyrics\n"
     ]
    }
   ],
   "source": [
    "from russian_songs_dataset_utils import process_music_ref_dataset\n",
    "\n",
    "base_ref_tracks = process_music_ref_dataset(\"/media/k4_nas/Datasets/Music_RU/Vocal_Dereverb\", cache_filename=\"ref_data_cache.json\")\n",
    "print(f\"found {len(base_ref_tracks)} base tracks with lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03.Encelt - Дикая охота\n",
      "/media/k4_nas/Datasets/Music_RU/Vocal_Dereverb/Encelt/Истории у костра [2017]/03.Encelt - Дикая охота.mir.json\n",
      "﻿Женский голос, отчётливый, легенды, фолк, рок, мюзикл, эпичный\n",
      "140\n",
      "how many sections: 7\n",
      "Между небом и землёй в дикой ясоте Слышишь ветер вой на протяжной ноте Мчатся вихрем в ночи,\n",
      "/media/k4_nas/Datasets/Music_RU/Vocal_Dereverb/Encelt/Истории у костра [2017]/03.Encelt - Дикая охота_vocals_stretched_120bpm_section2.mp3\n"
     ]
    }
   ],
   "source": [
    "ref_track_name = list(base_ref_tracks.keys())[515]\n",
    "ref_track = base_ref_tracks[ref_track_name]\n",
    "print(ref_track_name)\n",
    "print(ref_track['mir'])\n",
    "print(ref_track['caption'])\n",
    "print(ref_track['mir_data']['bpm'])\n",
    "print(f\"how many sections: {len(ref_track['sections'])}\")\n",
    "print(ref_track['sections'][0]['words'])\n",
    "print(ref_track['sections'][0]['mp3_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ref_audio = \"src/f5_tts/infer/examples/basic/basic_ref_en.wav\"\n",
    "# # ref_audio = \"/media/k4_nas/Datasets/Music_RU/Vocal_Dereverb/Tesla Boy/Андропов [2020]/05.Tesla Boy - Ватикан_vocals_stretched_120bpm_section5.mp3\"\n",
    "# # ref_text = \"Prosti moi drug, prosti moi drug, ne opravdal, ne opravdal, instinkti vrut, instinkti vrut, ya vidishny, kreatariseksual\"\n",
    "# ref_audio = \"/media/k4_nas/Datasets/Music_RU/Vocal_Dereverb/Tesla Boy/Андропов [2020]/05.Tesla Boy - Ватикан_vocals_stretched_120bpm_section5.mp3\"\n",
    "# ref_text = \"Prosti moi drug, prosti moi drug, ne opravdal, ne opravdal, instinkti vrut, instinkti vrut, ya vidishny, kreatariseksual\"\n",
    "# # gen_text = \"nichego na svete luchshe netu, chem brodit' druz'jam po belu svetu. tem, kto druzhen, ne strashny trevogi. nam ljubye dorogi dorogi\"\n",
    "# # gen_text = \"nichego na svete luchshe netu, chem brodit' druz'jam po belu svetu.\"\n",
    "\n",
    "def get_text_audio_and_end_time_path_for_ref(ref_id, section_num, base_ref_tracks):\n",
    "    ref_track = base_ref_tracks[list(base_ref_tracks.keys())[ref_id]]\n",
    "    words = ref_track['sections'][section_num]['words']\n",
    "    mp3_path =ref_track['sections'][section_num]['mp3_path']\n",
    "    return cyrtranslit.to_latin(words, \"ru\").lower(), mp3_path, ref_track['sections'][section_num]['end_time']\n",
    "\n",
    "def to_translit(text):\n",
    "    return cyrtranslit.to_latin(text, \"ru\").lower()\n",
    "\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(80, 1, base_ref_tracks) # 80 плохой\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(105, 2, base_ref_tracks) # тут нужна скорость 0.1\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(500, 2, base_ref_tracks) # тут нужна скорость 0.3\n",
    "ref_text, ref_audio, ref_end_time = get_text_audio_and_end_time_path_for_ref(600, 0, base_ref_tracks) # монеточка,  скорость 0.3\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(630, 0, base_ref_tracks) # какой-то женский шансон,  скорость 0.3\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(638, 0, base_ref_tracks) #  скорость 0.6\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(640, 0, base_ref_tracks) # Этот всё выговаривает, скорость 0.5\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(645, 0, base_ref_tracks) #  скорость 0.7\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(648, 0, base_ref_tracks) #  скорость 0.7\n",
    "# ref_text, ref_audio = get_text_audio_and_end_time_path_for_ref(680, 0, base_ref_tracks) # рэп, скорость 0.9\n",
    "# ref_text, ref_audio, ref_end_time = get_text_audio_and_end_time_path_for_ref(360, 0, base_ref_tracks) # \n",
    "\n",
    "gen_text = to_translit(\"Эх, полным полна коробочка! Есть в ней ситец и парча. Пожалей, душа-зазнобушка молодецкого плеча\")\n",
    "# gen_text = to_translit(\"О-о-о-о, зеленоеглазое такси. О-о-о-о, притормози, притормози! О-о-о-о, ты отвези меня туда! О-о-о-о, где будут рады мне всегда! \")\n",
    "# gen_text = to_translit(\"Ты сидишь на стуле с голым торсом, я смотрю на пиццу, у меня вопросы.\")\n",
    "# gen_text = to_translit(\"Эх, полным полна коробочка!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простая генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3 - main process call & visualization\n",
    "# speed = 0.8\n",
    "# print(f\"Symbols in ref text: {len(ref_text)}\")\n",
    "# print(f\"Symbols in gen text: {len(gen_text)}\")\n",
    "# main_process(ref_audio, ref_text, gen_text, ema_model, mel_spec_type, remove_silence, speed, end_time=ref_end_time, nfe_step=32, cfg_strength=3.0)\n",
    "# print(len(ref_text))\n",
    "\n",
    "# # Display mel-spectrograms\n",
    "# from IPython.display import display, Image, Audio\n",
    "\n",
    "# ref_mel_path = Path(output_dir) / \"ref_mel.png\"\n",
    "# gen_mel_path = Path(output_dir) / \"gen_mel.png\"\n",
    "\n",
    "# display(Image(filename=ref_mel_path))\n",
    "# display(Image(filename=gen_mel_path))\n",
    "\n",
    "# # Display audio\n",
    "# display(Audio(ref_audio))\n",
    "# display(Audio(wave_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частичная генерация с промежуточным растяжением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio...\n",
      "Audio is over 15s, clipping short. (2)\n",
      "Using cached reference text...\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image, Audio\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "\n",
    "# Parameters\n",
    "N = 5  # Step at which to intervene\n",
    "stretch_factor = 1.2  # How much to stretch along time axis\n",
    "total_steps = 32\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# First stage - generate trajectory up to step N\n",
    "ref_audio_preprocessed, ref_text_preprocessed = preprocess_ref_audio_text(ref_audio, ref_text) \n",
    "first_audio, sr, _, first_trajectory = infer_single_process(\n",
    "    ref_audio_preprocessed, ref_text_preprocessed, gen_text, ema_model, vocoder, \n",
    "    mel_spec_type=mel_spec_type, \n",
    "    nfe_step=total_steps,\n",
    "    speed=speed,\n",
    "    end_step=N\n",
    ")\n",
    "\n",
    "# Get intermediate state and remove noise according to schedule\n",
    "noise_scale = 1 - (N/total_steps) ** 2\n",
    "intermediate_state = first_trajectory[-1] - first_trajectory[0] * noise_scale\n",
    "\n",
    "# Draw mel spectrum after noise removal\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(intermediate_state.squeeze().cpu().numpy().T, aspect='auto', origin='lower', cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.title('After noise removal at step N')\n",
    "\n",
    "# # Get the audio and process it\n",
    "# audio, sr = torchaudio.load(ref_audio_preprocessed)\n",
    "# if audio.shape[0] > 1:\n",
    "#     audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "# audio = audio.to(device=device, dtype=torch.float32)\n",
    "\n",
    "# if sr != target_sample_rate:\n",
    "#     resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n",
    "#     resampler = resampler.to(device)\n",
    "#     audio = resampler(audio)\n",
    "\n",
    "# # Calculate mel conditioning\n",
    "# mel = vocoder.mel_spec(audio) if mel_spec_type == \"bigvgan\" else model_obj.mel_spec(audio)\n",
    "# cond = mel.permute(0, 2, 1)\n",
    "# cond_seq_len = cond.shape[1]\n",
    "\n",
    "# Stretch intermediate state\n",
    "orig_len = intermediate_state.shape[1]\n",
    "new_len = int(orig_len * stretch_factor)\n",
    "stretched_state = F.interpolate(\n",
    "    intermediate_state.permute(0, 2, 1), \n",
    "    size=new_len, \n",
    "    mode='linear', \n",
    "    align_corners=False\n",
    ").permute(0, 2, 1).to(device)\n",
    "\n",
    "# Handle conditioning - only stretch the non-padded part\n",
    "# real_cond = cond[:, :cond_seq_len, :]\n",
    "# stretched_cond = F.interpolate(\n",
    "#     real_cond.permute(0, 2, 1),\n",
    "#     size=int(cond_seq_len * stretch_factor),\n",
    "#     mode='linear',\n",
    "#     align_corners=False\n",
    "# ).permute(0, 2, 1)\n",
    "\n",
    "# Pad the stretched conditioning to match new length\n",
    "# stretched_cond = F.pad(\n",
    "#     stretched_cond,\n",
    "#     (0, 0, 0, new_len - stretched_cond.shape[1]),\n",
    "#     value=0.0\n",
    "# )\n",
    "\n",
    "# Add new noise scaled to the same amount we removed\n",
    "new_noise = torch.randn(\n",
    "    stretched_state.shape[0], \n",
    "    new_len,\n",
    "    stretched_state.shape[2], \n",
    "    device=device, \n",
    "    dtype=stretched_state.dtype\n",
    ") * noise_scale\n",
    "\n",
    "stretched_state_with_noise = stretched_state + new_noise\n",
    "\n",
    "# Continue flow from step N+1 with stretched state\n",
    "audio, sr, mel_spec, final_trajectory = infer_single_process(\n",
    "    ref_audio_preprocessed, ref_text_preprocessed, gen_text, ema_model, vocoder, \n",
    "    mel_spec_type=mel_spec_type, \n",
    "    nfe_step=total_steps,\n",
    "    speed=speed * stretch_factor,  # Adjust speed to match new length\n",
    "    start_step=N+1,\n",
    "    initial_state=stretched_state_with_noise\n",
    ")\n",
    "\n",
    "# Draw final mel spectrum\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mel_spec.T, aspect='auto', origin='lower', cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.title('Final result after full denoising')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save and play final audio\n",
    "sf.write(\"stretch_test.mp3\", audio, sr)\n",
    "display(Audio(\"stretch_test.mp3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
